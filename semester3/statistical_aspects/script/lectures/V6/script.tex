\section{V6}
\subsection{Lineare Regression}
\subsubsection{Modellannahme}
$y_i = \beta_0 + \beta_1 x_i + \epsilon_i$ mit $i \in \{1, ... , n\}$\\\\
Schätzen mit Kleinste Quadrate Methode: Regressionsgerade minimiert die Summe der quadrierten Abstände zwischen den y-Werten der Regressionsgerade und den y-Werten der Datenpunkte (Residuen)

\subsubsection{Schätzen der Betas („Intercept“ und „Slope“)}
Ableiten der Residuenquadratsumme nach den Betas und Nullsetzen.\\
$\hat{\beta_1}=\frac{\displaystyle \sum_{i=1}^{n} (x_i - \bar{x}_n)(y_i - \bar{y}_n)}{\displaystyle \sum_{i=1}^{n} (x_i - \bar{x}_n)^2}$\\\\
$\hat{\beta_0}= \bar{y}_n - \hat{\beta_1} \cdot x_n$\\\\

Mean square error (MSE) = $\frac{1}{n} \cdot RSS$\\
residual sum of squares (RSS) = $\displaystyle \sum_{i=1}^{n} (y_i - f(x_i))^2$

\subsubsection{Varianzzerlegung und erklärte Varianz bei linearer Regression}
$\displaystyle \underbrace{\sum_{i=1}^{n} (y_i - \bar{y})^2}_{\text{Gesamtvarianz}} = \displaystyle \underbrace{\sum_{i=1}^{n} (y_i - \hat{y_i})^2}_{\text{Varianz der Residuen}} + \displaystyle \underbrace{\sum_{i=1}^{n} (\hat{y_i} - \bar{y})^2}_{\text{Varianz der Regresswerte}}$

\newpage
\textbf{Erklärte Varianz (= Bestimmtheitsmaß $R^2$)}\\\\
$R^2= 1 - \displaystyle \frac{\text{Varianz der Residuen}}{\text{Gesamzvarianz}} =1- \frac{\displaystyle\sum_{i=1}^{n} (y_i - \hat{y_i})^2}{\displaystyle \sum_{i=1}^{n} (y_i - \bar{y})^2}$\\\\
$\displaystyle R^2= 1 - \frac{\text{Varianz der Regresswerte}}{\text{Gesamzvarianz}} = 1 - \frac{\displaystyle \sum_{i=1}^{n} (\hat{y_i} - \bar{y})^2}{\displaystyle \sum_{i=1}^{n} (y_i - \bar{y})^2}$
\\\\
Problem: Steigt die Anzahl der Parameter m im Modell, so steigt R2 unabhängig vom Einfluss der Variablen, deshalb Korrektur\\\\
\textbf{korrigiertes Bestimmtheitsmaß:}\\
$\bar{R}^2=1- \frac{\displaystyle \frac{1}{n-p} \displaystyle\sum_{i=1}^{n} (y_i - \hat{y_i})^2}{\displaystyle \frac{1}{n-1} \displaystyle \sum_{i=1}^{n} (y_i - \bar{y})^2}$\\
mit p=Anzahl der unabhängigen Variablen

\subsubsection{Multivariate Regression}
Wie lineare, aber mehrere betas

\subsubsection{AIC}
AIC (Akaike Informations Kriterium): je kleiner desto besser\\
$AIC=-2L+2m$
\\\\
Bayesianisches Informationskriterium (BIC): je kleiner desto besser\\
$BIC=-2L+mlog(n)$
\\\\
Log-Likelihood für Beide (bei linearer Regression):\\
$-L=\displaystyle \frac{n}{2}ln(2\pi \sigma^2) + \displaystyle \sum_{i=1}^{n} \frac{(\hat{y_i} - y_i)^2}{2\sigma^2}$\\

\subsection{Multivariate Regression}

\subsubsection{Schätzen von Kontrasten}
\textcolor{red}{Was soll das sein???}
\subsubsection{AIC}

\subsubsection{Interaktion}
\begin{itemize}
	\item möglich innerhalb multivariate Regression
	\item Berücksichtigung nicht-additiver Effekte (Modulation einer Einflußgröße durch eine andere)
\end{itemize}

z. B. multivariate lineare Regression:\\
$Y_i=\beta_0 + \beta_1 x_i^{(1)} + \beta_2 x_i^{(2)} + \beta_3 (x_i^{(1)} \cdot x_i^{(2)}) + \epsilon_i$
\\\\
$x_i^{(1)} \cdot x_i^{(2)}$ - Interaktionsterm:\\
$Y_i=\beta_0 + \underbrace{(\beta_1 + \beta_3 x_i^{(2)})}_{=: \tilde{\beta_1}=\tilde{\beta_2}(x_i^{(2)})}x_i^{(1)} + \beta_2 x_i^{(2)} + \epsilon_i$

\subsection{Auswahl einer passenden Regressionsmethode}
\textbf{Best-Subset-Selection}
\begin{itemize}
	\item betrachte alle $\binom{m}{k}$ Modelle
	\begin{itemize}
		\item m: Anzahl der Einflussvariablen
		\item k $\leq$ m: Anzahl der ausgewählten Einflussvariablen
	\end{itemize}
	\item bestimme für jedes k das beste Modell anhand RSS, MSE oder R$^2$
	\item wähle aus den k Modellen das beste Modell anhand AIC,
BIC, Adjusted R$^2$
	\item Problematisch für große m
	\item Die Regressionskoeffizienten sind bei diesem Vorgehen gebiast (vergleiche später Sequentialtest)
\end{itemize}

\textbf{Vorwärts-Selektion}
\begin{itemize}
	\item Beginn mit Null-Modell (keine Einflußvariablen)
	\item füge diejenige Einflußsvariable hinzu, bei der RSS, MSE oder R 2 am besten ist
	\item wähle aus den k Modellen das beste Modell anhand AIC, BIC, Adjusted R$^2$ oder MSE
\end{itemize}

\textbf{Rückwärts-Eliminierung}
\begin{itemize}
	\item Beginn mit vollständigem Modell (alle Einflußvariablen)
	\item entferne diejenige Ausgangsvariable, bei der RSS, MSE oder R$^2$ am besten ist
	\item wähle aus den k Modellen das beste Modell anhand AIC, BIC, Adjusted R$^2$ oder MSE
\end{itemize}

Idealerweise kommen hierbei die selben Modelle heraus (ist in der Praxis jedoch selten der Fall), auch hier sind die Schätzer der $\beta_j$ des besten Modells gebiast

\subsection{Aufgaben zur Übung 6}